<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Matthew Finlayson" />
  <title>Hello world, MathML in NetNewsWire</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="style/main.css" />
</head>
<body>
<header id="title-block-header">
<h1 class="title">Hello world, MathML in NetNewsWire</h1>
<p class="author">Matthew Finlayson</p>
</header>
<p>Short logs. No organization.</p>
<h2 id="section">2025-04-07</h2>
<p>This weekend I stumbled upon <a
href="https://arxiv.org/abs/2412.06769">Training Large Language Models
to Reason in a Continuous Latent Space</a> whose Figure 1 has striking
similarity to my <a href="2025-04-04.html">proposed architecture</a>
from last week.</p>
<figure>
<img src="files/recurrent-reasoning.svg"
alt="Proposed recurrent latent-space reasoning architecture from last week" />
<figcaption aria-hidden="true">Proposed recurrent latent-space reasoning
architecture from last week</figcaption>
</figure>
<figure>
<img src="img/coconut.png" alt="The Coconut architecture" />
<figcaption aria-hidden="true">The Coconut architecture</figcaption>
</figure>
<p>This is a nice confirmation of my intuitions!</p>
<h2 id="todays-reading">Today‚Äôs reading</h2>
<p><a href="https://arxiv.org/pdf/2204.06974">Planting Undetectable
Backdoors in Machine Learning Models</a>, which is related to my <a
href="2025-04-03.html">current work on token rankings as LM
signatures</a>.</p>
<h2 id="section-1">2025-04-04</h2>
<h2 id="recurrent-transformers-for-reasoning">Recurrent transformers for
reasoning</h2>
<figure>
<img src="files/recurrent-reasoning.svg"
alt="My proposed latent reasoning architecture" />
<figcaption aria-hidden="true">My proposed latent reasoning
architecture</figcaption>
</figure>
<p>I have a hunch that ‚Äúreasoning‚Äù LLMs don‚Äôt actually need natural
language intermediate reasoning steps. I‚Äôm imagining a recurrent latent
reasoning chain where the output at each step is the input to the next.
This would not be efficient for parallel training, but my understanding
is that these models are trained via RL anyways, which, as I understand
it, must be autoregressive.</p>
<p>I‚Äôve heard of techniques that allow LMs to use a ‚Äúthink‚Äù token or a
‚Äúbackspace‚Äù token. I‚Äôm also aware of a paper that used ‚Äú‚Ä¶‚Äù instead of
chain of thought. Here is a (lightly) annotated list of related
papers.</p>
<h3 id="looped-transformers">Looped Transformers</h3>
<p><a href="https://openreview.net/forum?id=2edigk8yoU">Looped
Transformers for Length Generalization</a> is a theoretical paper that
constructs length generalizing transformers using the ‚Äúlooped
transformer‚Äù architecture. <a
href="https://openreview.net/forum?id=fiHVIUkulb">Looped
Transformers</a> proposed the original architecture. This is a
theoretical paper that compares looped architecture aspects of Turing
machines.</p>
<h3 id="universal-transformers">Universal transformers</h3>
<p>The ‚Äúoriginal‚Äù recurrent transformer paper. Repeated application of
the same transformer block until ‚Äústopping condition‚Äù is met.</p>
<h3 id="deep-equilibrium-models"><a
href="https://arxiv.org/abs/1909.01377">Deep equilibrium models</a></h3>
<p>I really like this model idea. This is something I have thought about
in the past: repeated applying a transformation until it reaches a
stable state as a stopping condition. The implicit differentiation is a
cool trick that makes this possible. I wonder how this could work in a
transformer architecture where the hidden state output from the previous
step is used as the input to the next.</p>
<h2 id="converting-a-trained-transformer-to-an-rnn">Converting a trained
transformer to an RNN</h2>
<p>I did a class project on this, using polynomial fitting to try to get
a better initializaton for a linear RNN to replace the softmax attention
layer in a pretrained transformer. It didn‚Äôt work well, but I have some
ideas for what might work next.</p>
<ul>
<li>Replace the softmax layer with multiple, stacked linear RNN
layers.</li>
<li>Train an RNN to mimic the IO behavior of the KV cache.</li>
</ul>
<h2 id="section-2">2025-04-03</h2>
<h3 id="setting-up-pico.sh">Setting up pico.sh</h3>
<p><a href="pico.sh">pico.sh</a> seems like a cool, minimal effort tool
for blogging, but it doesn‚Äôt support math, so I‚Äôm going to roll my
own.</p>
<h3 id="next-research-project">Next research project</h3>
<p>After wrapping up my COLM submission, I want to get back into a
project that I left off on a couple months ago.</p>
<p>The idea is that token rankings act as a <a
href="https://en.wikipedia.org/wiki/Message_authentication_code">message
authentication code (MAC)</a> for LLM outputs in APIs. If you know the
model params, it is easy to produce and verify
top-<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>
ranking that could have come from a model. If you don‚Äôt have the model
params, I conjecture that it is NP-hard to do these things. Thus, the
params function as the secret key, and the
top-<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>
outputs function as the authentication code (known as the ‚Äútag‚Äù).</p>
<p>The NP hardness result is based on the fact that oriented matroid
(OM) realizability is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>‚àÉ</mo><mi>‚Ñù</mi></mrow><annotation encoding="application/x-tex">\exists\mathbb{R}</annotation></semantics></math>-complete,
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi>ùñ≠</mi><mi>ùñØ</mi></mrow><mo>‚â§</mo><mo>‚àÉ</mo><mi>‚Ñù</mi><mo>‚â§</mo><mrow><mi>ùñØ</mi><mi>ùñ≤</mi><mi>ùñØ</mi><mi>ùñ†</mi><mi>ùñ¢</mi><mi>ùñ§</mi></mrow></mrow><annotation encoding="application/x-tex">\mathsf{NP}\leq\exists\mathbb{R}\leq\mathsf{PSPACE}</annotation></semantics></math>
is a complexity class. My work so far has shown that if you can learn an
unembedding matrix that realizes the set of
top-<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>
a model outputs, then you can solve OM realizability, and vice versa,
meaning that stealing the unembedding matrix is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>‚àÉ</mo><mi>‚Ñù</mi></mrow><annotation encoding="application/x-tex">\exists\mathbb{R}</annotation></semantics></math>
complete. This is a nice assurance for API designers, since they know
that adversaries will not steal model parameters.</p>
<p>Having established that you cannot learn the embedding matrix from
the API, a follow-up question is whether you can learn an approximately
correct OM that matches sets of
top-<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>
tokens. To prove the negative, I want to show that you would need to see
an exponential number of rankings to eliminate alternative OM
candidates, and that the alternative OM candidates are problematic,
i.e., misclassify unseen rankings at a high rate. This is hard, since
each ranking can probably eliminate an exponential number of candidates
at each step. The other approach is to say that the OM representation
would need to be exponentially large to be learnable, which would cause
issues for the learner.</p>
<h2 id="section-3">2024-12-17</h2>
<p>Torch code bug that took me an hour to fix: I wrapped a function in
<code>@torch.inference_mode</code> which called another function which
called another function that was trying to call
<code>torch.backward</code>.</p>
<h3 id="cleaning-out-my-notes-app">Cleaning out my Notes app</h3>
<p>Booking this site <a href="https://luc.devroye.org/math.html">fonts
for mathematics</a> for later use.</p>
<p>Intrigued by the idea of building my own Bluesky feed using <a
href="https://bsky.app/profile/graze.social">Graze</a>. Maybe some day I
will have time to.</p>
<p>My colleague from USC pointed me to their <a
href="https://openreview.net/forum?id=eRkNNQRppH">incomplete paper</a>
about training dynamics that they submitted to ICLR. I mean to give it a
skim sometime.</p>
<h2 id="section-4">2024-12-05</h2>
<p>I created an RSS feed for my website. I did it by hand, <a
href="https://www.w3schools.com/xml/xml_rss.asp">this template</a>. I
don‚Äôt use any frameworks for maintaining my website, I just write
everything by hand. I don‚Äôt change my website often enough for it to be
worthwhile to do anything fancier.</p>
<p>I also realized that, at least for <a
href="netnewswire.com">NetNewsWire</a>, <a
href="https://mathml.igalia.com">MathML</a> is supported but <a
href="mathjax.org">MathJax</a> is not. I will be doing all my math in
MathML from now on (just use the <code>--mathml</code> flag when
converting to html with <code>pandoc</code>).
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><munderover><mo>‚àë</mo><mrow><mi>n</mi><mo>=</mo><mn>0</mn></mrow><mo accent="false">‚àû</mo></munderover><mfrac><msup><mi>x</mi><mi>n</mi></msup><mrow><mi>n</mi><mi>!</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\exp(x)=\sum_{n=0}^\infty \frac{x^n}{n!}</annotation></semantics></math></p>
</body>
</html>
