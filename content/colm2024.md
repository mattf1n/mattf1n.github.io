---
title: COLM posters I thought were interesting enough to take a picture of.
subtitle: There were definitely more that I didn't see.
author: "[Matthew Finlayson](index.html)"
---

------

![Model Autophagy](img/COLM2024IMG_4828.HEIC.jpg)

I've been looking for papers that engage with the whether model collapse is a problem for LLMs. This paper looks like it does.

![CA-LoRA](img/COLM2024IMG_4829.HEIC.jpg)

I hadn't thought about PEFT for compressed models before so this idea caught my attention and
I talked to these authors a bit. My understanding is that LoRA trained with a full model doesn't work so well when the the model gets compressed. So they train another LoRA with a non-linearity to correct the degradation.
Their explanation is that the correct update for a compressed model is no longer expressible in low-rank. 
I'm not convinced that this is the best way, it seems fundamentally inelegant to just add on more parameters, and it seems like there should be an equivalent PEFT method that adds no parameters.

![Counting Transformers](img/COLM2024IMG_4830.HEIC.jpg)

Cool paper from FLaNN people at COLM. I would be interested in seeing how well this would work if it were compiled to a transformer. Are there any C-RASP to transformer compilers?

![LLMs plan ahead?](img/COLM2024IMG_4831.HEIC.jpg)

I've always wondered about this. I'm glad someone studied it! Interesting distinction between "pre-caching" and "breakcrumbs". Breadcrumbs never seemed plausible to me. 

![Unforgettable Generalization](img/COLM2024IMG_4833.HEIC.jpg)

I talked to the authors about the mechanism for this phenomenon and it fits with an intuition I have about overparameterization in LLMs, that mechanisms/pathways within the model persist even after they are no longer used because the extra computation is "free".

![Stream of Search](img/COLM2024IMG_4834.HEIC.jpg)

Relates to our current work on masking gradients from tokens we don't want to learn to generate but are important for context. Relates because they don't mask gradients from these tokens BUT they told me some followup work tried it (maybe unpublished) and it didn't make a difference. Bodes poorly for us perhaps?

![Dependencies in speculative decoding](img/COLM2024IMG_4845.HEIC.jpg)

This just seems like an RNN. Do we already use RNNs for speculative decoding models?

![Linearizing LLMs](img/COLM2024IMG_4846.HEIC.jpg)

Another pretrained transformer to pretrained RNN recipe. Very relevant to our current project (except we want to initialize with a better softmax approximation).

![Interp tool generalization](img/COLM2024IMG_4854.HEIC.jpg)

I like these papers. Cool to see another one.

![Early wight averaging for high LR](img/COLM2024IMG_4855.HEIC.jpg)

Seems like a very general deep learning finding. I haven't worked on problems in this area before, but I am interested in model merging for a potential future project, which seems to be a critical component here.

![Infinigram](img/COLM2024IMG_4856.HEIC.jpg)

I just want an explanation of the datastructure used here.

![Contexts are not arrays](img/COLM2024IMG_4857.HEIC.jpg)

I want to give this one a closer read, but it seems like an interesting approach that could inform future improvements to long-context LLMs.

