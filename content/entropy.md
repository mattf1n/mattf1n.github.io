---
title: Decoding Strategies and Entropy
author: Matthew Finlayson
---

## Hypothesis

My hypothesis is that we can characterize the relationships between different decoding methods and hyperparameters by looking at their entropy. 
We can use these relationships to predict which decoding method is most appropriate for a task
(a task is a metric plus a set of instances over which to measure) 
without doing an expensive exhaustive hyperparameter sweep. 

The entropy of a decoding strategy with parameter $p$ is the expected entropy of the next-token distributions for all contexts sampled from a language. 
This can be estimated by finding the average next-token entropy over a corpus. 
<aside> 
This is not the only way to define the entropy of a decoding strategy,
and only makes sense for token-level decoding strategies like top-p or temperature.
MBR decoding is not token-level.
One might define a measure of decoding method entropy as the entropy over *generations*,
e.g., the expected log-probability of the *sequence* generated by the model.
See [this article](https://en.wikipedia.org/wiki/Entropy_estimation) 
for an overview of some of the approaches and challenges associated with estimating entropy.
</aside>

We are focused on entropy because this is measurable across decoding strategies.
Entropy captures how diverse a decoding method's generations are.
High entropy means diverse, low entropy means deterministic, or close to greedy.

What we want is a function 
that takes 
$$\arg\max_\texttt{param}\texttt{metric}(\texttt{param}\mid\texttt{strat},\texttt{task})$$
and gives a latent variable $\texttt{entropy}$
and a function that takes $\texttt{entropy}$
and gives
$$\arg\max_\texttt{param}\texttt{metric}(\texttt{param}\mid\texttt{strat},\texttt{task})$$ 
for each $\texttt{task}, \texttt{metric}$.
In other words, we want an invertible function for each strategy that translates optimal parameters into a latent variable which fully describes the entropy level of the task,
agnostic of which decoding method is being used.

The first step of our experiments should be to try to see if such a function exists.
To do this we might plot optimal parameters for two different decoding methods against each other. The x-axis should be hyperparams from method $a$, y-axis hyperparams from method $b$.
Data points should represent different tasks.
To find a data point, do a hyperparam sweep for both methods on a task, 
identify the best hyperparam for both methods,
enter the data point on the plot.
We can then see if there is a pattern.

After we have done this, we can turn our attention to predicting the peformance of the methods.
