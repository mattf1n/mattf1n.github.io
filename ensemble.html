<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Matthew Finlayson" />
  <title>The “Right Way” to Ensemble Language Models</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">The “Right Way” to Ensemble Language Models</h1>
<p class="author"><a href="index.html">Matthew Finlayson</a></p>
</header>
<p>Suppose you have <span class="math inline">\(n\)</span> langauge
models with embedding size <span class="math inline">\(d\)</span>,
vocabulary size <span class="math inline">\(v\)</span>, and softmax
matrices <span class="math inline">\(\boldsymbol W_1, \boldsymbol
W_2,\ldots,\boldsymbol W_n\in\mathbb{R}^{v\times d}\)</span> and you
want to sample from them as an ensemble. One naive way to accomplish
this would be to average their logits <span
class="math inline">\(\boldsymbol\ell_1,\boldsymbol\ell_2,\ldots,\boldsymbol\ell_n\)</span>
and sample from <span class="math display">\[\boldsymbol{\hat{p}} =
\mathrm{softmax}\left(\sum_{i=1}^n\boldsymbol\ell_i / n\right).\]</span>
This presumably averages out distributional errors from individual
models.</p>
<p>But what if we assume that the models’ errors only come from the
softmax bottleneck? Using the techniques from <a
href="http://arxiv.org/abs/2310.01693">my recent paper</a> it is
possible to exactly pinpoint the set of possible true distributions as
the solutions <span class="math inline">\(\boldsymbol{p}\)</span> that
satisfy the constraints <span
class="math display">\[\boldsymbol{p}\boldsymbol{W}_i =
\boldsymbol{\hat{p}}_i\boldsymbol{W}_i\]</span> for <span
class="math inline">\(i=1,2,\ldots,n\)</span> where <span
class="math inline">\(\boldsymbol{\hat{p}}_i\)</span> is model <span
class="math inline">\(i\)</span>’s next-token distribution. Note that if
<span class="math inline">\(nd=v\)</span> then there is only one such
solution, and if <span class="math inline">\(nd&gt;v\)</span> there will
be no such solution, in which case we might opt for the least squares
solution.</p>
<p>In the case where <span class="math inline">\(nd &lt; v\)</span>
there will be many solutions, and it is not clear to me which solution
to choose here. Since the models are trained to minimize cross entropy
with the true distribution we can choose a solution that minimizes the
sum of the models’ cross entropy <span
class="math display">\[\boldsymbol{p}^\ast = \arg\min_\boldsymbol{p}
-\sum_{i=1}^n\boldsymbol{p}^\top\log\boldsymbol{\hat{p}}_i,\]</span>
which can be found via linear programming. If we don’t want any one
models’ cross entropy to be high, I believe we can also solve for <span
class="math display">\[\boldsymbol{p}^\ast = \arg\min_\boldsymbol{p}
\sum_{i=1}^n(\boldsymbol{p}^\top\log\boldsymbol{\hat{p}}_i)^2\]</span>
using quadratic programming. Note that without the linear constraints,
these solutions would simply be the one-hot vector indicating <span
class="math display">\[\arg\min_i-\sum_{j=1}^n\log\hat{p}_{ij}.\]</span>
Alternative approaches might be to minimize the under-estimation of the
token log-probabilities, or minimize the true probability distributions
cross entropy with the predicted distributions. It is not clear to me
yet whether any of these strategies or assumptions are equivalent or
better than one another.</p>
</body>
</html>
