<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Matthew Finlayson's personal website.">
    <title>Matt Fin</title>
    <link rel="apple-touch-icon" sizes="180x180" href="img/fin-180.png">
    <link rel="icon" type="image/png" sizes="32x32" href="img/fin-32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="img/fin-16.png">
    <link rel="manifest" href="favicon/site.webmanifest">
    <link rel="stylesheet" href="style/main.css">
  </head>

  <body>
    <header>
      <img src='img/profile3.jpg'>
      <h1 id="matthew-finlayson">Matthew Finlayson</h1>
      <address>
        mattbnfin at gmail dot com
      </address>
    </header>
    <nav>
      <ul>
        <li><a href="files/cv.pdf">CV</a></li>
        <li><a href='https://twitter.com/mattf1n'>Twitter</a></li>
        <li><a href='https://bsky.app/profile/mattf1n.bsky.social'>Bluesky</a></li>
        <li><a href="https://scholar.google.com/citations?user=37YtY2EAAAAJ&hl=en&oi=ao">Google Scholar</a></li>
        <li><a href="https://www.semanticscholar.org/author/Matthew-Finlayson/1580418311">Semantic&nbsp;Scholar</a></li>
        <li><a href='https://github.com/mattf1n'>GitHub</a></li>
      </ul>
    </nav>
    <main>
      <h2 id=About>About</h2>
      <p>
      Hello!
      I am a PhD student at USC, advised by Swabha Swa&shy;yam&shy;dip&shy;ta and Xiang Ren.
      Previously, I was a Predoctoral Researcher at AI2, 
      and before that I studied computer science and linguistics at Harvard. 
      </p>
      <p>
      My current research focuses on improving language modeling, sampling, and interpretability methods
      by building and exploiting our theoretical understanding of neural language models.
      </p>
      <h2 id=News>News</h2>
      <table>
        <!-- <tr> -->
        <!--   <td><time>Jun&nbsp;2024</time></td> -->
        <!--   <td>Joined Meta GenAI as a summer research intern.</td> -->
        <!-- </tr> -->
        <tr>
          <td><time>Apr&nbsp;2024</time></td>
          <td>Gave at <a href="files/lll.pdf">talk</a> at Meta FAIR on stealing ChatGPT's hidden size.</td>
        </tr>
        <tr>
          <td><time>Jan&nbsp;2024</time></td>
          <td>Gave at <a href="files/ccc.pdf">talk</a> at CMU LTI on decoding and the softmax bottleneck.</td>
        </tr>
        <tr>
          <td><time>Jan&nbsp;2024</time></td>
          <td>Paper accepted to ICLR.</td>
        </tr>
        <tr>
          <td><time>Oct&nbsp;2023</time></td>
          <td>Paper accepted to EMNLP.</td>
        </tr>
        <tr>
          <td><time>Aug&nbsp;2023</time></td>
          <td>Joined USC as a PhD student in NLP.</td>
        </tr>
        <tr>
          <td><time>Mar&nbsp;2023</time></td>
          <td>Selected for NSF GRFP Honorable Mention.</td>
        </tr>
        <tr>
          <td><time>Feb&nbsp;2023</time></td>
          <td>Gave a <a href="files/math.pdf">talk</a> at IST/Unbabel on math reasoning evaluation.</td>
        </tr>
        <tr>
          <td><time>Jan&nbsp;2023</time></td><td><q>Decomposed Prompting</q> accepted to ICLR.</td>
        </tr>
        <tr>
          <td><time>Nov&nbsp;2022</time></td>
          <td>
            Gave a <a href=files/instructions.pdf>talk</a> 
            at <a href=https://flann.super.site>FLaNN</a> 
            on using formal languages to studying instruction learning.
          </td>
        </tr>
        <tr>
          <td><time>Oct&nbsp;2022</time></td><td>Two papers accepted to EMNLP.</td>
        </tr>
        <tr>
          <td><time>Aug&nbsp;2021</time></td><td>Joined AI2 as a pre-doctoral researcher.</td>
        </tr>
      </table>
      <h2 id=posts>Posts</h2>
      <ul>
        <li><a href="ensemble.html">The "right way" to ensemble language models.</a></li>
        <li><a href="differentiable-binary-to-onehot.html">A differentiable function from binary to one-hot representations.</a></li>
        <li><a href="deep-ba-sampling.html">Deep BA sampling (extending BAT).</a></li>
        <li><a href="interest-demo.html">Research interest demos for working with me.</a></li>
        <li><a href="openlogprobs.html">Obtaining logprobs from an LLM API.</a></li>
        <li><a href="smislinear.html">The softmax function is linear.</a></li>
        <li><a href="gallery.html">Visualizations</a></li>
      </ul>
      <h2 id=Software>Software</h2>
      <ul>
        <li><a href="https://github.com/justinchiu/openlogprobs">OpenLogProbs</a>: a library for obtaining logprobs from API-protected language models.</li>
        <li><a href="https://github.com/mattf1n/ss">SS.py</a>: my personal command line tool for searching and citing academic papers via Semantic Scholar.</li>
      </ul>
      <h2 id=Publications>Publications and preprints</h2>
      <ol>
        <li>
          <q>Logits of API-Protected LLMs Leak Proprietary Information</q>.<br>
          <strong>Matthew Finlayson</strong>, Xiang Ren, and Swabha Swa&shy;yam&shy;dip&shy;ta.<br>
          <cite>ArXiv</cite>, <time>2024</time>.
          <small>
            <a href="https://arxiv.org/abs/2403.09539">PDF</a>
          </small>
          <details>
            <summary>Abstract</summary>
            <p>
            The commercialization of large language models (LLMs) has led to the
            common practice of restricting access to proprietary models via a
            limited API. In this work we show that, with only a conservative
            assumption about the model architecture, it is possible to learn a
            surprisingly large amount of non-public information about an
            API-protected LLM from a relatively small number of API queries (e.g.,
            costing under $1000 USD for OpenAI’s <code>gpt-3.5-turbo</code>). Our
            findings are centered on one key observation: most modern LLMs suffer
            from a softmax bottleneck, which restricts the model outputs to a linear
            subspace of the full output space. We exploit this fact to unlock
            several capabilities, including (but not limited to) obtaining cheap
            full-vocabulary outputs, auditing for specific types of model updates,
            identifying the source LLM given a single full LLM output, and even
            efficiently discovering the LLM’s hidden size. Our empirical
            investigations show the effectiveness of our methods, which allow us to
            estimate the embedding size of OpenAI’s <code>gpt-3.5-turbo</code> to be
            about 4096. Lastly, we discuss ways that LLM providers can guard against
            these attacks, as well as how these capabilities can be viewed as a
            feature (rather than a bug) by allowing for greater transparency and
            accountability.
            </p>
          </details>
        </li>
        <li>
          <q>Closing the Curious Case of Neural Text Degeneration</q>.<br>
          <strong>Matthew Finlayson</strong>, John Hewitt, Alexander Koller, Swabha Swa&shy;yam&shy;dip&shy;ta, and Ashish Sabharwal.<br>
          <cite>ICLR</cite>, <time>2024</time>.
          <small>
            <a href="http://arxiv.org/abs/2310.01693">PDF</a>
            <a href="https://github.com/mattf1n/basis-aware-threshold">Code</a>
          </small>
          <details>
            <summary>Abstract</summary>
            <p>Despite their ubiquity in language generation, it remains unknown why
            truncation sampling heuristics like nucleus sampling are so effective.
            We provide a theoretical explanation for the effectiveness of the
            truncation sampling by proving that truncation methods that discard
            tokens below some probability threshold (the most common type of
            truncation) can guarantee that all sampled tokens have nonzero true
            probability. However, thresholds are a coarse heuristic, and necessarily
            discard some tokens with nonzero true probability as well. In pursuit of
            a more precise sampling strategy, we show that we can leverage a known
            source of model errors, the softmax bottleneck, to prove that certain
            tokens have nonzero true probability, without relying on a threshold.
            Based on our findings, we develop an experimental truncation strategy
            and the present pilot studies demonstrating the promise of this type of
            algorithm. Our evaluations show that our method outperforms its
            threshold-based counterparts under automatic and human evaluation
            metrics for low-entropy (i.e., close to greedy) open-ended text
            generation. Our theoretical findings and pilot experiments provide both
            insight into why truncation sampling works, and make progress toward
            more expressive sampling algorithms that better surface the generative
            capabilities of large language models.</p>
          </details>
        </li>
        <li>
          <q>Attentiveness to Answer Choices Doesn't Always Entail High QA Accuracy</q>.<br>
          Sarah Wiegreffe, <strong>Matthew Finlayson</strong>, Oyvind Tafjord, 
          Peter Clark, and Ashish Sabharwal.<br>
          <cite>EMNLP</cite>, <time>2023</time>.
          <small>
            <a href="https://arxiv.org/abs/2305.14596">PDF</a>
            <a href="https://github.com/allenai/revisiting_surface_form_competition">Code</a>
          </small>
        </li>
        <li>
          <q>Decomposed Prompting: A Modular Approach for Solving Complex Tasks</q>.<br>
          Tushar Khot, Harsh Trivedi, <strong>Matthew Finlayson</strong>, Yao Fu,
          Kyle Richardson, Peter Clark and Ashish Sabharwal.<br>
          <cite>ICLR</cite>, <time>2023</time>.
          <small>
            <a href="https://arxiv.org/abs/2210.02406">PDF</a>
            <a href="https://github.com/allenai/DecomP">Code</a>
          </small>
        </li>
        <li>
          <q>L&imacr;la: A Unified Benchmark for Mathematical Reasoning</q>.<br>
          {<strong>Matthew Finlayson</strong>, Swaroop Mishra,} 
          Pan Lu, Leonard Tang, Sean Welleck,
          Chitta Baral, Tanmay Rajpurohit, Oyvind Tafjord, 
          Ashish Sabharwal, Peter Clark, and Ashwin Kalyan.<br>
          <cite>EMNLP</cite>, <time>2022</time>.
          <small>
            <a href="https://arxiv.org/abs/2210.17517">PDF</a> 
            <a href="https://github.com/allenai/Lila">Data</a>
            <a href="https://huggingface.co/allenai/bhaskara">Model</a>
            <a href="https://lila.apps.allenai.org">Website</a>
            <a href="https://leaderboard.allenai.org/lila/submissions/public">Leaderboard</a>
          </small>
        </li>
        <li>
          <q>What Makes Instruction Learning Hard? 
            An Investigation and a New Challenge in a Synthetic Environment</q>.<br> 
          <strong>Matthew Finlayson</strong>, Kyle Richardon, Ashish Sabharwal, and Peter Clark.<br>
          <cite>EMNLP</cite>, <time>2022</time>.
          <small>
            <a href="https://arxiv.org/abs/2204.09148">PDF</a> 
            <a href="https://github.com/allenai/RegSet">Code</a>
          </small>
        </li>
        <li>
          <q>Causal Analysis of Syntactic Agreement Mechanisms in Neural Language Models</q>.<br>
          {<strong>Matthew Finlayson</strong>, Aaron Mueller,}
          Sebastian Gehrmann, Stuart Shieber, Tal Linzen, and Yonatan Belinkov.<br>
          <cite>ACL</cite>, <time>2021</time>.
          <small>
            <a href="https://aclanthology.org/2021.acl-long.144/">PDF</a> 
            <a href="https://github.com/mattf1n/lm-intervention">Code</a>
          </small>
        </li>
      </ol>
    </main>
    <footer><img src="img/fin.png"></footer>
  </body>
</html>
