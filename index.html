<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Matthew Finlayson's personal website.">
    <title>Matt Fin</title>
    <link rel="apple-touch-icon" sizes="180x180" href="img/fin-180.png">
    <link rel="icon" type="image/png" sizes="32x32" href="img/fin-32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="img/fin-16.png">
    <link rel="manifest" href="favicon/site.webmanifest">
    <link rel="stylesheet" href="style/main.css">
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  </head>

  <body>
    <div id=sidebar>
      <header>
        <img src='img/profile3.jpg'>
        <h1 id="matthew-finlayson">Matthew Finlayson</h1>
        <address>
          mattbnfin at gmail dot com
        </address>
      </header>
      <nav>
        <ul>
          <li><a href="files/matthew_finlayson.pdf">CV</a></li>
          <li><a href='https://bsky.app/profile/mattf1n.bsky.social'>Bluesky</a></li>
          <li><a href='https://twitter.com/mattf1n'>Twitter</a></li>
          <li><a href="https://scholar.google.com/citations?user=37YtY2EAAAAJ&hl=en&oi=ao">Google Scholar</a></li>
          <li><a href="https://www.semanticscholar.org/author/Matthew-Finlayson/1580418311">Semantic&nbsp;Scholar</a></li>
          <li><a href='https://github.com/mattf1n'>GitHub</a></li>
        </ul>
      </nav>
    </div>
    <main>
      <h1>Hello!</h1>
      <p>
      I am a PhD student at USC, advised by Swabha Swayamdipta and Xiang Ren.
      Previously, I was a Predoctoral Researcher at AI2, 
      and before that I studied computer science and linguistics at Harvard. 
      </p>
      <p>
      My current research focuses on improving language modeling, sampling, and interpretability methods
      by building and exploiting our theoretical understanding of neural language models.
      </p>
      <h1>News</h1>
      <table>
        <tr>
          <td><time>Oct&nbsp;2023</time></td>
          <td><q>Closing the Curious Case &hellip;</q> preprint released.</td>
        </tr>
        <tr>
          <td><time>Oct&nbsp;2023</time></td>
          <td><q>Attentiveness to Answer Choices &hellip;</q> accepted to EMNLP.</td>
        </tr>
        <tr>
          <td><time>Apr&nbsp;2023</time></td>
          <td>Will join USC as a PhD student in NLP this fall.</td>
        </tr>
        <tr>
          <td><time>Mar&nbsp;2023</time></td>
          <td>Selected for NSF GRFP Honorable Mention.</td>
        </tr>
        <tr>
          <td><time>Feb&nbsp;2023</time></td>
          <td>Gave a <a href="files/math.pdf">talk</a> at IST/Unbabel on math reasoning evaluation.</td>
        </tr>
        <tr>
          <td><time>Jan&nbsp;2023</time></td><td><q>Decomposed Prompting</q> accepted to ICLR.</td>
        </tr>
        <tr>
          <td><time>Nov&nbsp;2022</time></td>
          <td>
            Gave a <a href=files/instructions.pdf>talk</a> 
            at <a href=https://flann.super.site>FLaNN</a> 
            on studying instruction learning with formal languages.
          </td>
        </tr>
        <tr>
          <td><time>Oct&nbsp;2022</time></td><td>2 papers accepted to EMNLP.</td>
        </tr>
        <tr>
          <td><time>Aug&nbsp;2021</time></td><td>Joined AI2 as a pre-doctoral researcher.</td>
        </tr>
      </table>
      <h1>Publications and preprints</h1>
      <ol>
        <li>
          <q>Closing the Curious Case of Neural Text Degeneration</q>.<br>
          <strong>Matthew Finlayson</strong>, John Hewitt, Alexander Koller, Swabha Swayamdipta, and Ashish Swbharwal.<br>
          <cite>ArXiv</cite>, <time>2023</time>.<br>
          <small>
            <a href="http://arxiv.org/abs/2310.01693">[PDF]</a>
          </small>
        </li>
        <li>
          <q>Attentiveness to Answer Choices Doesn't Always Entail High QA Accuracy</q>.<br>
          Sarah Wiegreffe, <strong>Matthew Finlayson</strong>, Oyvind Tafjord, 
          Peter Clark, and Ashish Sabharwal.<br>
          <cite>EMNLP</cite>, <time>2023</time>.<br>
          <small>
            <a href="https://arxiv.org/abs/2305.14596">[PDF]</a>
          </small>
        </li>
        <li>
          <q>Decomposed Prompting: A Modular Approach for Solving Complex Tasks</q>.<br>
          Tushar Khot, Harsh Trivedi, <strong>Matthew Finlayson</strong>, Yao Fu,
          Kyle Richardson, Peter Clark and Ashish Sabharwal.<br>
          <cite>ICLR</cite>, <time>2023</time>.<br>
          <small>
            <a href="https://arxiv.org/abs/2210.02406">[PDF]</a>
            <a href="https://github.com/allenai/DecomP">[Code]</a>
          </small>
        </li>
        <li>
          <q>L&imacr;la: A Unified Benchmark for Mathematical Reasoning</q>.<br>
          {<strong>Matthew Finlayson</strong>, Swaroop Mishra,} 
          Pan Lu, Leonard Tang, Sean Welleck,
          Chitta Baral, Tanmay Rajpurohit, Oyvind Tafjord, 
          Ashish Sabharwal, Peter Clark, and Ashwin Kalyan.<br>
          <cite>EMNLP</cite>, <time>2022</time>.<br>
          <small>
            <a href="https://arxiv.org/abs/2210.17517">[PDF]</a> 
            <a href="https://github.com/allenai/Lila">[Data]</a>
            <a href="https://huggingface.co/allenai/bhaskara">[Model]</a>
            <a href="https://lila.apps.allenai.org">[Website]</a>
            <a href="https://leaderboard.allenai.org/lila/submissions/public">[Leaderboard]</a>
          </small>
        </li>
        <li>
          <q>What Makes Instruction Learning Hard? 
            An Investigation and a New Challenge in a Synthetic Environment</q>.<br> 
          <strong>Matthew Finlayson</strong>, Kyle Richardon, Ashish Sabharwal, and Peter Clark.<br>
          <cite>EMNLP</cite>, <time>2022</time>.<br>
          <small>
            <a href="https://arxiv.org/abs/2204.09148">[PDF]</a> 
            <a href="https://github.com/allenai/RegSet">[Code]</a>
          </small>
        </li>
        <li>
          <q>Causal Analysis of Syntactic Agreement Mechanisms in Neural Language Models</q>.<br>
          {<strong>Matthew Finlayson</strong>, Aaron Mueller,}
          Sebastian Gehrmann, Stuart Shieber, Tal Linzen, and Yonatan Belinkov.<br>
          <cite>ACL</cite>, <time>2021</time>.<br>
          <small>
            <a href="https://aclanthology.org/2021.acl-long.144/">[PDF]</a> 
            <a href="https://github.com/mattf1n/lm-intervention">[Code]</a></p>
          </small>
        </li>
      </ol>
      <h1>Software</h1>
      <ul>
        <li><a href="https://github.com/justinchiu/openlogprobs">OpenLogProbs</a>: a library for obtaining logprobs from API-protected language models.</li>
        <li><a href="https://github.com/mattf1n/ss">SS.py</a>: my personal command line tool for searching and citing academic papers via Semantic Scholar.</li>
      </ul>
      <h1>Other</h1>
      <ul>
        <li><a href="openlogprobs.html">Obtaining logprobs from an LLM API.</a></li>
        <li><a href="smislinear.html">The softmax function is linear.</a></li>
        <li><a href="gallery.html">Visualizations</a></li>
      </ul>
    </main>
    <footer><img src="img/fin.png"></footer>
  </body>
</html>
