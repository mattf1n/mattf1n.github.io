<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Matthew Finlayson" />
  <meta name="dcterms.date" content="2025-12-08" />
  <title>Some papers that caught my attention at NeurIPS 2025</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <link rel="stylesheet" href="../style/blog.css" />
  <link rel="apple-touch-icon" sizes="180x180" href="/img/fin-180.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/img/fin-32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/img/fin-16.png">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Some papers that caught my attention at NeurIPS
2025</h1>
<p class="author"><a href="/index.html">Matthew Finlayson</a></p>
<p class="date">2025-12-08</p>
</header>
<p><a href="https://arxiv.org/abs/2506.19004">Broken Tokens? Your
Language Model can Secretly Handle Non-Canonical Tokenizations</a> shows
that language models are fine with however you tokenize your doc. Can we
hide information in the tokenization scheme (Alisa’s idea at poster)
that might allow jailbreaking at test time/fingerprinting? How do LMs
learn not to attend to “New” in “New York”? (York token encodes all
info). Does the model “know” when it is getting non-canonical
tokenizations? What does the model do with the extra compute from
inefficient tokenizations?</p>
<p><a href="https://arxiv.org/abs/2504.03790">Sample, Don’t Search:
Rethinking Test-Time Alignment for Language Models</a> gives an
algorithm that, in the limit, samples from an RL-tuned model without
actually doing the RL. All it needs is a reward model and base language
model. Works best with continuous rewards and when the RL training would
narrow the distribution from the base model. From one of Andre Martin’s
former students now doing a PhD with Noah Smith.</p>
<p><a href="https://horwitz.ai/model-atlas">We Should Chart an Atlas of
All the World’s Models</a> gives a pretty visualization of model family
trees. May be useful for model forensics work.</p>
</body>
</html>
