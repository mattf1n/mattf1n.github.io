<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Matthew Finlayson" />
  <title>Research Interest Demo</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Research Interest Demo</h1>
<p class="author"><a href="index.html">Matthew Finlayson</a></p>
</header>
<p>Hello! It is great to hear that you are interested in doing research
:). I love doing research and mentoring people when I have the time.
Time and effort are precious limited resources both for mentors and
mentees, and I want to respect that for both of us. In order to gauge
our fit I ask that you do a “demonstration of interest,” i.e., a small
demo project/writeup.</p>
<p>An interest demo might consist of:</p>
<ul>
<li>A high-level project proposal that outlines the what, how, and
why.</li>
<li>A hypothesis, goal, mathematical idea that could help us better
understand some aspect of language or language modeling</li>
<li>The results for an initial experiment, with an interpretation and
ideas for next steps.</li>
</ul>
<p>The point of the demo is to show me that you are able to carry out a
mostly-independent research project, so I will look for evidence that
you have the time, commitment, and required skills to think about
interesting problems and hypotheses and engineer experiments to test
them. Of course you may have not done research before, so feel free to
reach out to me with incomplete demos to get feedback on what might be
interesting or worthwhile. Once you have a complete demo, I would be
happy to meet to discuss how to move forward. I will do my best to
provide guidance as best I can.</p>
<h1 id="research-ideas">Research ideas</h1>
<p>Some interesting questions, directions, and project kernels. I will
add and revise this list periodically. You do not need to choose a topic
from this list, but hopefully this can help give you some ideas. You can
also look for inspiration among my <a
href="index.html#posts">posts</a>.</p>
<h2 id="pretraining-data-detection">Pretraining data detection</h2>
<p>There are existing methods for identifying which data a language
model has been trained on. Is it possible to provide guarantees on the
accuracy of such methods? Can we come up with new, more effective
methods? Can the softmax bottleneck provide any leverage for this?</p>
<h2 id="evaluating-decoding-in-language-models">Evaluating decoding in
language models</h2>
<p>How can we reliably tell which sampling method is better (e.g.,
top-k, top-p) for a language model on a task? Is there a mathematically
justified way to accurately compare these methods in a fair way? One
ideas I have thrown around is an adversarial setting where each decoding
method must sample from the set difference of the two methods. However,
I’m not sure if this is a fair comparison.</p>
<h2 id="error-allocation-for-language-models">Error allocation for
language models</h2>
<p>When a language model suffers from a “softmax bottleneck”, how do
they allocate errors? Is there a way to mathematically characterize
which tokens will have larger errors than others? My paper “Closing the
Curious Case of Neural Text Degeneration” deals with some of these
questions, but some of the mathematical ideas could be further
developed.</p>
<p>For instance, during the project I thought I might be able to show
that SM bottleneck causes log-prob underestimation errors that are not
arbitrarily close to 0, but I never got to figuring out if it could be
done. I also hypothesize that models will have much larger absolute
probability errors on high-probability tokens, but larger absolute
log-probability errors on low-probability tokens, but I have not worked
out any formal mathematical justification. Lastly, I wonder if models
are more likely to underestimate high-probability tokens and
overestimate low-probability tokens.</p>
<h2 id="language-model-embeddings">Language model embeddings</h2>
<p>How do language models choose their embeddings? Some preliminary
experiments suggest that embeddings tend to reside near the surface of a
small-radius hypersphere. How could this geometric phenomenon be used to
our advantage?</p>
<p>Can the particular choice of embeddings and their linear dependencies
be used to identify text generated by a language model with guaranteed
accuracy?</p>
</body>
</html>
